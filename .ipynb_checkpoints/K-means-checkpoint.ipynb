{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is computationally difficult(NP-hard); however, there are heuristic algorithm that are commonly employed and converge quickly to a local optimum.\n",
    "\n",
    "These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian Mixture Modeling. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of observations (x1, x2, …, xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k (≤ n) sets S = {S1, S2, …, Sk} so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:\n",
    "\n",
    "\\begin{equation*}\n",
    "argmin \\sum_{i=1}^{k}\\sum_{x in S_i}\\norm [2]{x - u_i}^2\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is a heuristic algorithm, there is no guarantee that it will converge to the global optimum, and the result may depend on the initial clusters. As the algorithm is usually very fast, it is common to run it multiple times with different starting conditions. However, in the worst case, k-means can be very slow to converge: in particular it has been shown that there exist certain point sets, even in 2 dimensions, on which k-means takes exponential time, that is 2Ω(n), to converge.[12] These point sets do not seem to arise in practice: this is corroborated by the fact that the smoothed running time of k-means is polynomial.[13]\n",
    "\n",
    "The \"assignment\" step is also referred to as expectation step, the \"update step\" as maximization step, making this algorithm a variant of the generalized expectation-maximization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If k and d (the dimension) are fixed, the problem can be exactly solved in time $${\\displaystyle O(n^{dk+1})}$$, where n is the number of entities to be clustered[19]\n",
    "Thus, a variety of heuristic algorithms such as Lloyd's algorithm given above are generally used.\n",
    "\n",
    "The running time of Lloyd's algorithm (and most variants) is O(nkdi),[20][21] where n is the number of d-dimensional vectors, k the number of clusters and i the number of iterations needed until convergence. \n",
    "\n",
    "Lloyd's algorithm is the standard approach for this problem, However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naive implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd's algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three key features of k-means which make it efficient are often regarded as its biggest drawbacks:\n",
    "\n",
    "1. Euclidean distance is used as a metric and variance is used as a measure of cluster scatter. So it tends to create equal size clusters. while The Gaussian models used by the Expectation-maximization algorithm (which can be seen as a generalization of k-means) are more flexible here by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than k-means as well as correlated clusters (not in this example).and since it depends on euclidean distance, **it dosen't apply to non-numerical data**. Since it depends on euclidean distance, it's not a good metric in high dimensions\n",
    "\n",
    "\n",
    "[O]ur intuitions, which come from a three-dimensional world, often do not apply in high-dimensional ones. In high dimensions, most of the mass of a multivariate Gaussian distribution is not near the mean, but in an increasingly distant “shell” around it; and most of the volume of a high-dimensional orange is in the skin, not the pulp. If a constant number of examples is distributed uniformly in a high-dimensional hypercube, beyond some dimensionality most examples are closer to a face of the hypercube than to their nearest neighbor. And if we approximate a hypersphere by inscribing it in a hypercube,\n",
    ". It is the same situation happened in Nearest Neighbor. How to solve it?\n",
    "\n",
    "first, check the effective dimensions, do a PCA and see how the eigenvalue drops. \n",
    "second, Thus, for a given problem with a fixed (high) value for the dimensionality d, it may be preferable to use lower values of k. This means that the L1 distance metric (Manhattan distance metric) is the most preferable for high dimensional applications, followed by the Euclidean metric (L2). ...\n",
    "\n",
    "It's like the kNN balls are too sparse to be helpful at probing a smooth hyperplane. With higher dimensions they feel increasingly more lonely.\n",
    "\n",
    "On the other hand, methods like SVM have a global view and do much better.\n",
    "\n",
    "2. euclidean distance defined similairy has assumption that clusters **are spatially grouped or spherical, iid guassian with same variance. it has similar variance at all dimensions, and it tends to cluster that all clusters have similar number of points**\n",
    "\n",
    "3. number of clusters k is an input, cross validation to check reasonable right k is important.\n",
    "\n",
    "4. convergence to a local minimum may produce counterintuitive results\n",
    "\n",
    "5. no prior informatoin, bad for unbalanced data\n",
    "\n",
    "6. it is also sensetive to scale, unit measurement. Sometimes, non-linear scaling or transformation might also need to be applied. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means other application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationship with other ML algorithm\n",
    "\n",
    "1. Gaussian mixture model and EM algorithm\n",
    "\n",
    "the inherint assumption of Kmeans is that cluster is a sperical distributed. in reality, gaussian distribution is more common.\n",
    "\n",
    "In kmeans, the prior distribution assumption is that all cluster has the similar number of samples\n",
    "\n",
    "Kmeans use euclidean distance to measure the similarity, but in GMM, it is measured using posterior probability, covariance matrix can allow us to give priority to different dimension\n",
    "\n",
    "\n",
    "Mixture of Gaussians\n",
    "在\"Dempster, A. P., N. M. Laird, and D. B. Rubin. \"Maximum Likelihood from Incomplete Data via the EM Algorithm.\" 1977\"，作者提出了著名的EM算法（Expectation-Maximization）。在EM的应用部分，作者介绍了利用EM算法来求解混合高斯模型（GMM，Gaussian Mixture Model）的参数。 \n",
    "\n",
    "\n",
    "在GMM中，GMM 的概率密度函数由 K 个 Gaussian 分布线性组成，如下。因此，利用ＧＭＭ来聚类，本质就是对数据集用ＧＭＭ拟合，并估计参数。求出的K个Gaussian就对应着K个类别。\n",
    "\n",
    "\n",
    "以下简要介绍利用ＥＭ算法估计GMM参数的过程：\n",
    "1.对于N个数据点，K个Gaussian分布，其联合概率密度可以表示为 \n",
    "\n",
    "\n",
    "2.对于数据x_{i}，它属于第k个类别的概率为\n",
    "\n",
    "\n",
    "3.利用最大似然估计，可以得到每个Gaussian的参数为。其中N_{k}=\\sum_{i=1}^{N}{\\gamma (i,k)} ，并且\\pi _{k}=N_{k}/N\n",
    "\n",
    "4.重复步骤2~3，直到似然函数的值收敛为止\n",
    "\n",
    "\n",
    "M是我一直想深入学习的算法之一，第一次听说是在NLP课中的HMM那一节，为了解决HMM的参数估计问题，使用了EM算法。在之后的MT中的词对齐中也用到了。在Mitchell的书中也提到EM可以用于贝叶斯网络中。\n",
    "\n",
    "http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html\n",
    "\n",
    "\n",
    "2. PCA\n",
    "\n",
    "with high dimension, the point are becoming very far away form each other, so use PCA first then do k-means\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9cecb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class KMeansClassifier():\n",
    "\t\"this is a k-means classifier\"\n",
    "\tdef __init__(self, k = 3, initCent = 'random', max_iter = 500):\n",
    "\t\tself._k = k\n",
    "\t\tself._initCent = initCent\n",
    "\t\tself._max_iter = max_iter\n",
    "\t\tself._clusterAssment = None\n",
    "\t\tself._labels = None\n",
    "\t\tself._sse = None\n",
    "\n",
    "\tdef _calEDist(self, arrA, arrB):\n",
    "\t\treturn np.math.sqrt(sum(np.power(arrA - arrB, 2)))\n",
    "\n",
    "\tdef _calMDist(self, arrA, arrB):\n",
    "\t\treturn sum(np.abs(arrA - arrB))\n",
    "\n",
    "\tdef _randCent(self, data_X, k):\n",
    "\t\t\"\"\"\n",
    "\t\toutput: k*m matrix of random centroid\n",
    "\t\t\"\"\"\n",
    "\t\tm = data_X.shape[1]\n",
    "\t\tcentroids = np.empty((k, m))\n",
    "\t\tfor j in range(m):\n",
    "\t\t\tminJ = min(data_X[:,j])\n",
    "\t\t\trangeJ = float(max(data_X[:, j] - minJ))\n",
    "\t\t\tcentroids[:, j] = (minJ + rangeJ*np.random.rand(k, 1)).flatten()\n",
    "\t\treturn centroids\n",
    "\n",
    "\tdef loadDataset(self, infile):\n",
    "\t\tdf = []\n",
    "\t\tfileIn = open(infile)  \n",
    "\t\tfor line in fileIn.readlines():  \n",
    "\t\t\tlineArr = line.strip().split()  \n",
    "\t\t\tdf.append([float(lineArr[0]), float(lineArr[1])])\n",
    "\t\tdf = pd.DataFrame(df)\n",
    "\t\treturn np.array(df).astype(np.float)\n",
    "\n",
    "\tdef fit(self, data_X):\n",
    "\t\t\"\"\"\n",
    "\t\tinput n*m matrix\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(data_X, np.ndarray) or isinstance(data_X, np.matrixlib.defmatrix.matrix):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tdata_X = np.asarray(data_X)\n",
    "\t\t\texcept:\n",
    "\t\t\t\traise TypeError(\"numpy.ndarray resuired for data_X\")\n",
    "\n",
    "\t\tn = data_X.shape[0]\n",
    "\t\tself._clusterAssment = np.zeros((n, 2))\n",
    "\t\tif self._initCent == 'random':\n",
    "\t\t\tself._centroids = self._randCent(data_X, self._k)\n",
    "\n",
    "\t\tclusterChanged = True\n",
    "\t\tfor _ in range(self._max_iter):\n",
    "\t\t\tclusterChanged = False\n",
    "\t\t\t# loop over each data point\n",
    "\t\t\tfor i in range(n):\n",
    "\t\t\t\tminDist = np.inf\n",
    "\t\t\t\tminIndex = -1\n",
    "\t\t\t\tfor j in range(self._k):\n",
    "\t\t\t\t\t# loop over each centroid\n",
    "\t\t\t\t\tarrA = self._centroids[j,:]\n",
    "\t\t\t\t\tarrB = data_X[i,:]\n",
    "\t\t\t\t\tdistJI = self._calEDist(arrA, arrB)\n",
    "\t\t\t\t\tif distJI < minDist:\n",
    "\t\t\t\t\t\tminDist = distJI\n",
    "\t\t\t\t\t\tminIndex = j\n",
    "\t\t\t\tif self._clusterAssment[i, 0] != minIndex or self._clusterAssment[i, 1] > minDist**2:\n",
    "\t\t\t\t\tclusterChanged = True\n",
    "\t\t\t\t\tself._clusterAssment[i, :] = minIndex, minDist**2\n",
    "\t\t\tif not clusterChanged:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tfor i in range(self._k):\n",
    "\t\t\t\tindex_all = self._clusterAssment[:, 0]\n",
    "\t\t\t\tvalue = np.nonzero(index_all == i)\n",
    "\t\t\t\tptsInClust = data_X[value[0]]\n",
    "\t\t\t\tself._centroids[i,:] = np.mean(ptsInClust, axis = 0)\n",
    "\n",
    "\t\tself._labels = self._clusterAssment[: , 0]\n",
    "\t\tself._sse = sum(self._clusterAssment[:, 1])\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\tif not isinstance(X, np.ndarray):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tX = np.asarray(X)\n",
    "\t\t\texcept:\n",
    "\t\t\t\traise TypeError(\"numpy.ndarray required for X\")\n",
    "\n",
    "\t\tn = X.shape[0]\n",
    "\t\tpreds = np.zeros((n,))\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tminDist = np.inf\n",
    "\t\t\tfor j in range(self._k):\n",
    "\t\t\t\tdistJI = self._calEDist(X[i,:], self._centroids[j,])\n",
    "\t\t\t\tif distJI < minDist:\n",
    "\t\t\t\t\tminDist = distJI\n",
    "\t\t\t\t\tpreds[i] = j\n",
    "\t\treturn preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\tk = 4\n",
    "\tclf = KMeansClassifier(k)\n",
    "\tdata_X = clf.loadDataset(\"C:/Users/zihao.zhang/Desktop/some questions/some-questions/kmenastestdata.txt\")\n",
    "\tclf.fit(data_X)\n",
    "\tcents = clf._centroids\n",
    "\tlabels = clf._labels\n",
    "\tsse = clf._sse\n",
    "\tcolors = ['b','g','r','k','c','m','y','#e24fff','#524C90','#845868']\n",
    "\tfor i in range(k):\n",
    "\t\tindex = np.nonzero(labels==i)[0]\n",
    "\t\tx0 = data_X[index, 0]\n",
    "\t\tx1 = data_X[index, 1]\n",
    "\t\ty_i = i\n",
    "\t\tfor j in range(len(x0)):\n",
    "\t\t\tplt.text(x0[j], x1[j], str(y_i), color=colors[i], \\\n",
    "\t\t\t\t\t\tfontdict={'weight': 'bold', 'size': 6})\n",
    "\t\tplt.scatter(cents[i,0],cents[i,1],marker='x',color=colors[i],\\\n",
    "\t\t\t\t\tlinewidths=7)\n",
    "\t\n",
    "\tplt.title(\"SSE={:.2f}\".format(sse))\n",
    "\tplt.axis([-7,7,-7,7])\n",
    "\toutname = \"./result/k_clusters\" + str(k) + \".png\"\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization:\n",
    "\n",
    "K-means clustering is \"isotropic\" in all directions of space and therefore tends to produce more or less round (rather than elongated) clusters. In this situation leaving variances unequal is equivalent to putting more weight on variables with smaller variance.\n",
    "\n",
    "From a philosophical point of view, standardization does not really solve the problem. Indeed, the choice of measurement units gives rise to relative weights of the variables. Expressing a variable in smaller units will lead to a larger range for that variable, which will then have a large effect on the resulting structure. On the other hand, by standardizing one attempts to give all variables an equal weight, in the hope of achieving objectivity. As such, it may be used by a practitioner who possesses no prior knowledge. However, it may well be that some variables are intrinsically more important than others in a particular application, and then the assignment of weights should be based on subject-matter knowledge (see, e.g., Abrahamowicz, 1985). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosine similarity and NLP\n",
    "\n",
    "The vector contains mostly zeros in NLP.\n",
    "euclidean distance in this example seemed to slightly relate to the length of the document?\n",
    "\n",
    "**Cosine Similarity is a way to measure overlap**\n",
    "\n",
    "\n",
    "\n",
    "Cosine similarity is generally used as a metric for measuring distance when the magnitude of the vectors does not matter. This happens for example when working with text data represented by word counts. We could assume that when a word (e.g. science) occurs more frequent in document 1 than it does in document 2, that document 1 is more related to the topic of science. However, it could also be the case that we are working with documents of uneven lengths (Wikipedia articles for example). Then, science probably occurred more in document 1 just because it was way longer than document 2. Cosine similarity corrects for this.\n",
    "\n",
    "在维度高的数据里面，某个点的距离他的最近点和最远点的距离接近对方。在数据科学里面是高纬度诅咒的一个部分(curse of dimension). 简单解释高纬度诅咒:随着维度的提升，很多我们在低纬度认为相当然的现象，在高纬度空间里面都不成立了:比如我们这里提到相邻之间的点全部都变得很远，远到基本上大家都差不多远哈哈。这个现象在余弦相似性和欧式距离里面都存在。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = (\n",
    "\"The sky is blue\",\n",
    "\"The sun is bright\",\n",
    "\"The sun in the sky is bright\",\n",
    "\"We can see the shining sun, the bright sun\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print (tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x11 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the TF-IDF matrix (tfidf_matrix) for each document (the number of rows of the matrix) with 11 tf-idf terms (the number of columns from the matrix), we can calculate the Cosine Similarity between the first document (“The sky is blue”) with each of the other documents of the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.36651513,  0.52305744,  0.13448867]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram 利用统计预测下一个词出现的概率\n",
    "用word2vec平均\n",
    "topic modeling "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
