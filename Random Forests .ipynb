{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T20:40:59.431345Z",
     "start_time": "2018-05-16T20:40:59.429345Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Python 2 / 3 compatability\n",
    "from __future__ import print_function\n",
    "# Toy dataset.\n",
    "# Format: each row is an example.\n",
    "# The last column is the label.\n",
    "# The first two columns are features.\n",
    "# Feel free to play with it by adding more features & examples.\n",
    "# Interesting note: I've written this so the 2nd and 5th examples\n",
    "# have the same features, but different labels - so we can see how the\n",
    "# tree handles this case.\n",
    "training_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Apple'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "]\n",
    "# Column labels.\n",
    "# These are used only to print the tree.\n",
    "header = [\"color\", \"diameter\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T20:41:07.804345Z",
     "start_time": "2018-05-16T20:41:07.779345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Green', 'Red', 'Yellow'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unique_vals(rows, col):\n",
    "    \"\"\"Find the unique values for a column in a dataset.\"\"\"\n",
    "    return set([row[col] for row in rows])\n",
    "#######\n",
    "# Demo:\n",
    "unique_vals(training_data, 0)\n",
    "# unique_vals(training_data, 1)\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T20:41:28.673345Z",
     "start_time": "2018-05-16T20:41:28.671345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple': 2, 'Grape': 2, 'Lemon': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def class_counts(rows):\n",
    "    \"\"\"Counts the number of each type of example in a dataset.\"\"\"\n",
    "    counts = {}  # a dictionary of label -> count.\n",
    "    for row in rows:\n",
    "        # in our dataset format, the label is always the last column\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "#######\n",
    "# Demo:\n",
    "class_counts(training_data)\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T20:41:47.643545Z",
     "start_time": "2018-05-16T20:41:47.641545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_numeric(value):\n",
    "    \"\"\"Test if a value is numeric.\"\"\"\n",
    "    return isinstance(value, int) or isinstance(value, float)\n",
    "#######\n",
    "# Demo:\n",
    "is_numeric(7)\n",
    "# is_numeric(\"Red\")\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T20:42:10.248745Z",
     "start_time": "2018-05-16T20:42:10.236745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is diameter >= 3?"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Question:\n",
    "    \"\"\"A Question is used to partition a dataset.\n",
    "\n",
    "    This class just records a 'column number' (e.g., 0 for Color) and a\n",
    "    'column value' (e.g., Green). The 'match' method is used to compare\n",
    "    the feature value in an example to the feature value stored in the\n",
    "    question. See the demo below.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "\n",
    "    def match(self, example):\n",
    "        # Compare the feature value in an example to the\n",
    "        # feature value in this question.\n",
    "        val = example[self.column]\n",
    "        if is_numeric(val):\n",
    "            return val >= self.value\n",
    "        else:\n",
    "            return val == self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        # This is just a helper method to print\n",
    "        # the question in a readable format.\n",
    "        condition = \"==\"\n",
    "        if is_numeric(self.value):\n",
    "            condition = \">=\"\n",
    "        return \"Is %s %s %s?\" % (\n",
    "            header[self.column], condition, str(self.value))\n",
    "    \n",
    "\n",
    "    \n",
    "##############\n",
    " # Demo:# Demo:\n",
    " # Let's write a question for a numeric attribute# Let's  \n",
    "Question(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:05:27.180945Z",
     "start_time": "2018-05-16T21:05:27.178945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How about one for a categorical attribute\n",
    "q = Question(0, 'Green')\n",
    "q\n",
    "# Let's pick an example from the training set...\n",
    "example = training_data[0]\n",
    "# ... and see if it matches the question\n",
    "q.match(example) # this will be true, since the first example is Green.\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:05:50.846545Z",
     "start_time": "2018-05-16T21:05:50.845545Z"
    }
   },
   "outputs": [],
   "source": [
    "def partition(rows, question):\n",
    "    \"\"\"Partitions a dataset.\n",
    "\n",
    "    For each row in the dataset, check if it matches the question. If\n",
    "    so, add it to 'true rows', otherwise, add it to 'false rows'.\n",
    "    \"\"\"\n",
    "    true_rows, false_rows = [], []\n",
    "    for row in rows:\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "    return true_rows, false_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:05:58.529745Z",
     "start_time": "2018-05-16T21:05:58.520745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Red', 1, 'Grape'], ['Red', 1, 'Grape']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "# Let's partition the training data based on whether rows are Red.\n",
    "true_rows, false_rows = partition(training_data, Question(0, 'Red'))\n",
    "# This will contain all the 'Red' rows.\n",
    "true_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:06:56.714945Z",
     "start_time": "2018-05-16T21:06:56.704945Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(rows):\n",
    "    \"\"\"Calculate the Gini Impurity for a list of rows.\n",
    "\n",
    "    There are a few different ways to do this, I thought this one was\n",
    "    the most concise. See:\n",
    "    https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\n",
    "    \"\"\"\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl**2\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:07:07.317345Z",
     "start_time": "2018-05-16T21:07:07.315345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "# Let's look at some example to understand how Gini Impurity works.\n",
    "#\n",
    "# First, we'll look at a dataset with no mixing.\n",
    "no_mixing = [['Apple'],\n",
    "              ['Apple']]\n",
    "# this will return 0\n",
    "gini(no_mixing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:07:21.553945Z",
     "start_time": "2018-05-16T21:07:21.542945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999999999999998"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we'll look at a dataset with many different labels\n",
    "lots_of_mixing = [['Apple'],\n",
    "                  ['Orange'],\n",
    "                  ['Grape'],\n",
    "                  ['Grapefruit'],\n",
    "                  ['Blueberry']]\n",
    "# This will return 0.8\n",
    "gini(lots_of_mixing)\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:07:38.612945Z",
     "start_time": "2018-05-16T21:07:38.598945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6399999999999999"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def info_gain(left, right, current_uncertainty):\n",
    "    \"\"\"Information Gain.\n",
    "\n",
    "    The uncertainty of the starting node, minus the weighted impurity of\n",
    "    two child nodes.\n",
    "    \"\"\"\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    return current_uncertainty - p * gini(left) - (1 - p) * gini(right)\n",
    "\n",
    "# Demo:\n",
    "# Calculate the uncertainy of our training data.\n",
    "current_uncertainty = gini(training_data)\n",
    "current_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:08:08.458145Z",
     "start_time": "2018-05-16T21:08:08.457145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1399999999999999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much information do we gain by partioning on 'Green'?\n",
    "true_rows, false_rows = partition(training_data, Question(0, 'Green'))\n",
    "info_gain(true_rows, false_rows, current_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:08:34.588745Z",
     "start_time": "2018-05-16T21:08:34.587745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37333333333333324"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What about if we partioned on 'Red' instead?\n",
    "true_rows, false_rows = partition(training_data, Question(0,'Red'))\n",
    "info_gain(true_rows, false_rows, current_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:08:43.094345Z",
     "start_time": "2018-05-16T21:08:43.089345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Red', 1, 'Grape'], ['Red', 1, 'Grape']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It looks like we learned more using 'Red' (0.37), than 'Green' (0.14).\n",
    "# Why? Look at the different splits that result, and see which one\n",
    "# looks more 'unmixed' to you.\n",
    "true_rows, false_rows = partition(training_data, Question(0,'Red'))\n",
    "\n",
    "# Here, the true_rows contain only 'Grapes'.\n",
    "true_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:09:24.393945Z",
     "start_time": "2018-05-16T21:09:24.376945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is diameter >= 3?"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_best_split(rows):\n",
    "    \"\"\"Find the best question to ask by iterating over every feature / value\n",
    "    and calculating the information gain.\"\"\"\n",
    "    best_gain = 0  # keep track of the best information gain\n",
    "    best_question = None  # keep train of the feature / value that produced it\n",
    "    current_uncertainty = gini(rows)\n",
    "    n_features = len(rows[0]) - 1  # number of columns\n",
    "\n",
    "    for col in range(n_features):  # for each feature\n",
    "\n",
    "        values = set([row[col] for row in rows])  # unique values in the column\n",
    "\n",
    "        for val in values:  # for each value\n",
    "\n",
    "            question = Question(col, val)\n",
    "\n",
    "            # try splitting the dataset\n",
    "            true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "            # Skip this split if it doesn't divide the\n",
    "            # dataset.\n",
    "            if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate the information gain from this split\n",
    "            gain = info_gain(true_rows, false_rows, current_uncertainty)\n",
    "\n",
    "            # You actually can use '>' instead of '>=' here\n",
    "            # but I wanted the tree to look a certain way for our\n",
    "            # toy dataset.\n",
    "            if gain >= best_gain:\n",
    "                best_gain, best_question = gain, question\n",
    "\n",
    "    return best_gain, best_question\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# Find the best question to ask first for our toy dataset.\n",
    "best_gain, best_question = find_best_split(training_data)\n",
    "best_question\n",
    "# FYI: is color == Red is just as good. See the note in the code above\n",
    "# where I used '>='.\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:19:17.623145Z",
     "start_time": "2018-05-16T21:19:17.621145Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    \"\"\"A Leaf node classifies data.\n",
    "\n",
    "    This holds a dictionary of class (e.g., \"Apple\") -> number of times\n",
    "    it appears in the rows from the training data that reach this leaf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = class_counts(rows)\n",
    "\n",
    "class Decision_Node:\n",
    "    \"\"\"A Decision Node asks a question.\n",
    "\n",
    "    This holds a reference to the question, and to the two child nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 true_branch,\n",
    "                 false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:19:18.887145Z",
     "start_time": "2018-05-16T21:19:18.885145Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_tree(rows):\n",
    "    \"\"\"Builds the tree.\n",
    "\n",
    "    Rules of recursion: 1) Believe that it works. 2) Start by checking\n",
    "    for the base case (no further information gain). 3) Prepare for\n",
    "    giant stack traces.\n",
    "    \"\"\"\n",
    "\n",
    "    # Try partitioing the dataset on each of the unique attribute,\n",
    "    # calculate the information gain,\n",
    "    # and return the question that produces the highest gain.\n",
    "    gain, question = find_best_split(rows)\n",
    "\n",
    "    # Base case: no further info gain\n",
    "    # Since we can ask no further questions,\n",
    "    # we'll return a leaf.\n",
    "    if gain == 0:\n",
    "        return Leaf(rows)\n",
    "\n",
    "    # If we reach here, we have found a useful feature / value\n",
    "    # to partition on.\n",
    "    true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "    # Recursively build the true branch.\n",
    "    true_branch = build_tree(true_rows)\n",
    "\n",
    "    # Recursively build the false branch.\n",
    "    false_branch = build_tree(false_rows)\n",
    "\n",
    "    # Return a Question node.\n",
    "    # This records the best feature / value to ask at this point,\n",
    "    # as well as the branches to follow\n",
    "    # dependingo on the answer.\n",
    "    return Decision_Node(question, true_branch, false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:19:20.553145Z",
     "start_time": "2018-05-16T21:19:20.550145Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_tree(node, spacing=\"\"):\n",
    "    \"\"\"World's most elegant tree printing function.\"\"\"\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        print (spacing + \"Predict\", node.predictions)\n",
    "        return\n",
    "\n",
    "    # Print the question at this node\n",
    "    print (spacing + str(node.question))\n",
    "\n",
    "    # Call this function recursively on the true branch\n",
    "    print (spacing + '--> True:')\n",
    "    print_tree(node.true_branch, spacing + \"  \")\n",
    "\n",
    "    # Call this function recursively on the false branch\n",
    "    print (spacing + '--> False:')\n",
    "    print_tree(node.false_branch, spacing + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:19:21.864545Z",
     "start_time": "2018-05-16T21:19:21.859545Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_tree = build_tree(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:19:22.518945Z",
     "start_time": "2018-05-16T21:19:22.517945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is diameter >= 3?\n",
      "--> True:\n",
      "  Is color == Yellow?\n",
      "  --> True:\n",
      "    Predict {'Apple': 1, 'Lemon': 1}\n",
      "  --> False:\n",
      "    Predict {'Apple': 1}\n",
      "--> False:\n",
      "  Predict {'Grape': 2}\n"
     ]
    }
   ],
   "source": [
    "print_tree(my_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:19:24.081345Z",
     "start_time": "2018-05-16T21:19:24.076345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple': 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify(row, node):\n",
    "    \"\"\"See the 'rules of recursion' above.\"\"\"\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.predictions\n",
    "\n",
    "    # Decide whether to follow the true-branch or the false-branch.\n",
    "    # Compare the feature / value stored in the node,\n",
    "    # to the example we're considering.\n",
    "    if node.question.match(row):\n",
    "        return classify(row, node.true_branch)\n",
    "    else:\n",
    "        return classify(row, node.false_branch)\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# The tree predicts the 1st row of our\n",
    "# training data is an apple with confidence 1.\n",
    "classify(training_data[0], my_tree)\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:19:55.938145Z",
     "start_time": "2018-05-16T21:19:55.936145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple': '100%'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_leaf(counts):\n",
    "    \"\"\"A nicer way to print the predictions at a leaf.\"\"\"\n",
    "    total = sum(counts.values()) * 1.0\n",
    "    probs = {}\n",
    "    for lbl in counts.keys():\n",
    "        probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n",
    "    return probs\n",
    "\n",
    "#######\n",
    "# Demo:\n",
    "# Printing that a bit nicer\n",
    "print_leaf(classify(training_data[0], my_tree))\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:20:14.525945Z",
     "start_time": "2018-05-16T21:20:14.524945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple': '50%', 'Lemon': '50%'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "# On the second example, the confidence is lower\n",
    "print_leaf(classify(training_data[1], my_tree))\n",
    "#######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T21:20:25.619745Z",
     "start_time": "2018-05-16T21:20:25.608745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: Apple. Predicted: {'Apple': '100%'}\n",
      "Actual: Apple. Predicted: {'Apple': '50%', 'Lemon': '50%'}\n",
      "Actual: Grape. Predicted: {'Grape': '100%'}\n",
      "Actual: Grape. Predicted: {'Grape': '100%'}\n",
      "Actual: Lemon. Predicted: {'Apple': '50%', 'Lemon': '50%'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate\n",
    "testing_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 4, 'Apple'],\n",
    "    ['Red', 2, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "]\n",
    "\n",
    "for row in testing_data:\n",
    "    print (\"Actual: %s. Predicted: %s\" %\n",
    "           (row[-1], print_leaf(classify(row, my_tree))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests - The Math of Intelligence (Week 7)\n",
    "\n",
    "\n",
    "## Our demo\n",
    "\n",
    "![alt text](https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/assets/blogimages/creditdecisiontree.png \"Logo Title Text 1\")\n",
    "\n",
    "We're going to learn about a machine learning model called a Random Forest. The task is to asses Credit Risk of someone using their financial history. Useful for insurance companies.\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)\n",
    "\n",
    "This dataset classifies people described by a set of attributes as good or bad credit risks.\n",
    "\n",
    "\n",
    "## What is a Random forest?\n",
    "\n",
    "### Based off of decision trees\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png \"Logo Title Text 1\")\n",
    "\n",
    "Classification and Regression Trees or CART for short is a term introduced by Leo Breiman to refer to Decision Tree algorithms that can used for classification or regression predictive modeling problems.\n",
    "\n",
    "The aim at each stage is to associate specific targets (i.e., desired output values) with specific values of a particular variable.The result is a decision-tree in which each path identifies a combination of values associated with a particular prediction.\n",
    "\n",
    "Each non-leaf node in this tree is basically a decision maker. These nodes are called decision nodes. Each node carries out a specific test to determine where to go next. Depending on the outcome, you either go to the left branch or the right branch of this node. We keep doing this until we reach a leaf node. If we are constructing a classifier, each leaf node represents a class. Let’s say you are trying to determine whether or not it’s going to rain tomorrow based on three factors available today — temperature, pressure, and wind. So a typical decision tree would look like this:\n",
    "\n",
    "![alt text](https://prateekvjoshi.files.wordpress.com/2016/03/2-tree.png \"Logo Title Text 1\")\n",
    "\n",
    "But how do we construct the optimal tree? What attribute should be at the root node? How do we decide the thresholds? \n",
    "\n",
    "We use the Gini Index as our cost function used to evaluate splits in the dataset. We minimize it.\n",
    "\n",
    "![alt text](http://i.imgur.com/IijgHbt.png \"Logo Title Text 1\")\n",
    "\n",
    "A split in the dataset involves one input attribute and one value for that attribute. It can be used to divide training patterns into two groups of rows.\n",
    "\n",
    "A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created by the split. A perfect separation results in a Gini score of 0, whereas the worst case split that results in 50/50 classes in each group results in a Gini score of 1.0 (for a 2 class problem).\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/decisiontree-151015165353-lva1-app6892/95/classification-using-decision-tree-41-638.jpg?cb=1444928106 \"Logo Title Text 1\")\n",
    "\n",
    "A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created \n",
    "by the split. A perfect separation results in a Gini score of 0, whereas the worst case split that \n",
    "results in 50/50 classes. We calculate it for every row and split the data accordingly in our binary tree. We repeat this process recursively. \n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/decisiontree-151015165353-lva1-app6892/95/classification-using-decision-tree-14-638.jpg?cb=1444928106\n",
    " \"Logo Title Text 1\")\n",
    " \n",
    "Using decision trees, we can build a random forest\n",
    " \n",
    "One problem that might occur with one big (deep) single DT is that it can overfit. That is the DT can “memorize” the training set the way a person might memorize an Eye Chart.\n",
    "\n",
    "The point of RF is to prevent overfitting. It does this by creating random subsets of the features and building smaller (shallow) trees using the subsets and then it combines the subtrees.\n",
    "\n",
    "The downside of RF is it can be slow if you have a single process but it can be parallelized.\n",
    "\n",
    "### Majority Vote\n",
    "![alt text](https://i.ytimg.com/vi/ajTc5y3OqSQ/hqdefault.jpg \"Logo Title Text 1\")\n",
    "\n",
    "### Subset of data\n",
    "![alt text](https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/assets/blogimages/sparkmlrandomforest.png \"Logo Title Text 1\")\n",
    "\n",
    "### Nodes represent feature splits\n",
    "![alt text](https://qph.ec.quoracdn.net/main-qimg-b17755d2e0ffb326d8c39b7f3e07e03b-c \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "## Other good examples?\n",
    "\n",
    "We can treat stock prices as a regression problem (time series) or classification\n",
    "\n",
    "#### Stock Price Classification (2 examples)\n",
    "\n",
    "https://github.com/qiulx026/stock-predictor-with-machine-learning-methods\n",
    "\n",
    "https://github.com/joy13/RandomForest\n",
    "\n",
    "Ignore the forest part for a moment, even a single tree can do regression. Each leaf holds a prediction value, which no longer is a class for regression. Given an input feature vector, you simply walk the tree as you'd do for a classification problem, and the resulting value in the leaf node is the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest Algorithm\n",
    "#This module implements pseudo-random number generators for various distributions.\n",
    "#seeding the generated number makes our results reproducible (good for debugging)\n",
    "from random import seed\n",
    "#Return a randomly selected element from range(start, stop, step). \n",
    "from random import randrange\n",
    "#read CSV file (dataset)\n",
    "from csv import reader\n",
    "#square root function\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    #init the dataset as a list\n",
    "\tdataset = list()\n",
    "    #open it as a readable file\n",
    "\twith open(filename, 'r') as file:\n",
    "        #init the csv reader\n",
    "\t\tcsv_reader = reader(file)\n",
    "        #for every row in the dataset\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "            #add that row as an element in our dataset list (2D Matrix of values)\n",
    "\t\t\tdataset.append(row)\n",
    "    #return in-memory data matrix\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    #iterate throw all the rows in our data matrix\n",
    "\tfor row in dataset:\n",
    "        #for the given column index, convert all values in that column to floats\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    #store a given column \n",
    "    class_values = [row[column] for row in dataset]\n",
    "    #create an unordered collection with no duplicates, only unique valeus\n",
    "    unique = set(class_values)\n",
    "    #init a lookup table\n",
    "    lookup = dict()\n",
    "    #for each element in the column\n",
    "    for i, value in enumerate(unique):\n",
    "        #add it to our lookup table\n",
    "        lookup[value] = i\n",
    "    #the lookup table stores pointers to the strings\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    #return the lookup table\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "# the original sample is randomly partitioned into k equal sized subsamples. \n",
    "#Of the k subsamples, a single subsample is retained as the validation data \n",
    "#for testing the model, and the remaining k − 1 subsamples are used as training data. \n",
    "#The cross-validation process is then repeated k times (the folds),\n",
    "#with each of the k subsamples used exactly once as the validation data.\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    #init 2 empty lists for storing split dataubsets\n",
    "\tleft, right = list(), list()\n",
    "    #for every row\n",
    "\tfor row in dataset:\n",
    "        #if the value at that row is less than the given value\n",
    "\t\tif row[index] < value:\n",
    "            #add it to list 1\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "            #else add it list 2 \n",
    "\t\t\tright.append(row)\n",
    "    #return both lists\n",
    "\treturn left, right\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    #how many correct predictions?\n",
    "\tcorrect = 0\n",
    "    #for each actual label\n",
    "\tfor i in range(len(actual)):\n",
    "        #if actual matches predicted label\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "            #add 1 to the correct iterator\n",
    "\t\t\tcorrect += 1\n",
    "    #return percentage of predictions that were correct\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    #folds are the subsamples used to train and validate model\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "    #for each subsample\n",
    "\tfor fold in folds:\n",
    "        #create a copy of the data\n",
    "\t\ttrain_set = list(folds)\n",
    "        #remove the given subsample\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "        #init a test set\n",
    "\t\ttest_set = list()\n",
    "        #add each row in a given subsample to the test set\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "        #get predicted labls\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "        #get actual labels\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "        #compare accuracy\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "        #add it to scores list, for each fold\n",
    "\t\tscores.append(accuracy)\n",
    "    #return all accuracy scores\n",
    "\treturn scores\n",
    " \n",
    " \n",
    "# Calculate the Gini index for a split dataset\n",
    "## this is the name of the cost function used to evaluate splits in the dataset.\n",
    "# this is a measure of how often a randomly chosen element from the set \n",
    "#would be incorrectly labeled if it was randomly labeled according to the distribution\n",
    "#of labels in the subset. Can be computed by summing the probability\n",
    "#of an item with label i being chosen times the probability \n",
    "#of a mistake in categorizing that item. \n",
    "#It reaches its minimum (zero) when all cases in the node \n",
    "#fall into a single target category.\n",
    "#A split in the dataset involves one input attribute and one value for that attribute. \n",
    "#It can be used to divide training patterns into two groups of rows.\n",
    "#A Gini score gives an idea of how good a split is by how mixed the classes \n",
    "#are in the two groups created by the split. A perfect separation results in \n",
    "#a Gini score of 0, whereas the worst case split that results in 50/50 classes \n",
    "#in each group results in a Gini score of 1.0 (for a 2 class problem).\n",
    "#We first need to calculate the proportion of classes in each group.\n",
    "def gini_index(groups, class_values):\n",
    "\tgini = 0.0\n",
    "    #for each class\n",
    "\tfor class_value in class_values:\n",
    "        #a random subset of that class\n",
    "\t\tfor group in groups:\n",
    "\t\t\tsize = len(group)\n",
    "\t\t\tif size == 0:\n",
    "\t\t\t\tcontinue\n",
    "            #average of all class values\n",
    "\t\t\tproportion = [row[-1] for row in group].count(class_value) / float(size)\n",
    "            #  sum all (p * 1-p) values, this is gini index\n",
    "\t\t\tgini += (proportion * (1.0 - proportion))\n",
    "\treturn gini\n",
    " \n",
    "# Select the best split point for a dataset\n",
    "#This is an exhaustive and greedy algorithm\n",
    "def get_split(dataset, n_features):\n",
    "    ##Given a dataset, we must check every value on each attribute as a candidate split, \n",
    "    #evaluate the cost of the split and find the best possible split we could make.\n",
    "\tclass_values = list(set(row[-1] for row in dataset))\n",
    "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "\tfeatures = list()\n",
    "\twhile len(features) < n_features:\n",
    "\t\tindex = randrange(len(dataset[0])-1)\n",
    "\t\tif index not in features:\n",
    "\t\t\tfeatures.append(index)\n",
    "\tfor index in features:\n",
    "\t\tfor row in dataset:\n",
    "            ##When selecting the best split and using it as a new node for the tree \n",
    "            #we will store the index of the chosen attribute, the value of that attribute \n",
    "            #by which to split and the two groups of data split by the chosen split point.\n",
    "            ##Each group of data is its own small dataset of just those rows assigned to the \n",
    "            #left or right group by the splitting process. You can imagine how we might split \n",
    "            #each group again, recursively as we build out our decision tree.\n",
    "\t\t\tgroups = test_split(index, row[index], dataset)\n",
    "\t\t\tgini = gini_index(groups, class_values)\n",
    "\t\t\tif gini < b_score:\n",
    "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    ##Once the best split is found, we can use it as a node in our decision tree.\n",
    "    ##We will use a dictionary to represent a node in the decision tree as \n",
    "    #we can store data by name. \n",
    "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    " \n",
    "# Create a terminal node value\n",
    "\n",
    "def to_terminal(group):\n",
    "    #select a class value for a group of rows. \n",
    "\toutcomes = [row[-1] for row in group]\n",
    "    #returns the most common output value in a list of rows.\n",
    "\treturn max(set(outcomes), key=outcomes.count)\n",
    " \n",
    "#Create child splits for a node or make terminal\n",
    "#Building a decision tree involves calling the above developed get_split() function over \n",
    "#and over again on the groups created for each node.\n",
    "#New nodes added to an existing node are called child nodes. \n",
    "#A node may have zero children (a terminal node), one child (one side makes a prediction directly) \n",
    "#or two child nodes. We will refer to the child nodes as left and right in the dictionary representation \n",
    "#of a given node.\n",
    "#Once a node is created, we can create child nodes recursively on each group of data from \n",
    "#the split by calling the same function again.\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    #Firstly, the two groups of data split by the node are extracted for use and \n",
    "    #deleted from the node. As we work on these groups the node no longer requires access to these data.\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups'])\n",
    "    \n",
    "    #Next, we check if either left or right group of rows is empty and if so we create \n",
    "    #a terminal node using what records we do have.\n",
    "\t# check for a no split\n",
    "\tif not left or not right:\n",
    "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
    "\t\treturn\n",
    "    #We then check if we have reached our maximum depth and if so we create a terminal node.\n",
    "\t# check for max depth\n",
    "\tif depth >= max_depth:\n",
    "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "\t\treturn\n",
    "    #We then process the left child, creating a terminal node if the group of rows is too small, \n",
    "    #otherwise creating and adding the left node in a depth first fashion until the bottom of \n",
    "    #the tree is reached on this branch.\n",
    "\t# process left child\n",
    "\tif len(left) <= min_size:\n",
    "\t\tnode['left'] = to_terminal(left)\n",
    "\telse:\n",
    "\t\tnode['left'] = get_split(left, n_features)\n",
    "\t\tsplit(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "\t# process right child\n",
    "    #The right side is then processed in the same manner, \n",
    "    #as we rise back up the constructed tree to the root.\n",
    "\tif len(right) <= min_size:\n",
    "\t\tnode['right'] = to_terminal(right)\n",
    "\telse:\n",
    "\t\tnode['right'] = get_split(right, n_features)\n",
    "\t\tsplit(node['right'], max_depth, min_size, n_features, depth+1)\n",
    " \n",
    "#Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    #Building the tree involves creating the root node and \n",
    "\troot = get_split(train, n_features)\n",
    "    #calling the split() function that then calls itself recursively to build out the whole tree.\n",
    "\tsplit(root, max_depth, min_size, n_features, 1)\n",
    "\treturn root\n",
    " \n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    #Making predictions with a decision tree involves navigating the \n",
    "    #tree with the specifically provided row of data.\n",
    "    #Again, we can implement this using a recursive function, where the same prediction routine is \n",
    "    #called again with the left or the right child nodes, depending on how the split affects the provided data.\n",
    "    #We must check if a child node is either a terminal value to be returned as the prediction\n",
    "    #, or if it is a dictionary node containing another level of the tree to be considered.\n",
    "\tif row[node['index']] < node['value']:\n",
    "\t\tif isinstance(node['left'], dict):\n",
    "\t\t\treturn predict(node['left'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['left']\n",
    "\telse:\n",
    "\t\tif isinstance(node['right'], dict):\n",
    "\t\t\treturn predict(node['right'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['right']\n",
    " \n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):\n",
    "\tsample = list()\n",
    "\tn_sample = round(len(dataset) * ratio)\n",
    "\twhile len(sample) < n_sample:\n",
    "\t\tindex = randrange(len(dataset))\n",
    "\t\tsample.append(dataset[index])\n",
    "\treturn sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees: 1\n",
      "Scores: [48.78048780487805, 60.97560975609756, 63.41463414634146, 58.536585365853654, 43.90243902439025]\n",
      "Mean Accuracy: 55.122%\n",
      "Trees: 5\n",
      "Scores: [53.65853658536586, 58.536585365853654, 60.97560975609756, 70.73170731707317, 63.41463414634146]\n",
      "Mean Accuracy: 61.463%\n",
      "Trees: 10\n",
      "Scores: [73.17073170731707, 63.41463414634146, 60.97560975609756, 53.65853658536586, 58.536585365853654]\n",
      "Mean Accuracy: 61.951%\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction with a list of bagged trees\n",
    "#responsible for making a prediction with each decision tree and \n",
    "#combining the predictions into a single return value. \n",
    "#This is achieved by selecting the most common prediction \n",
    "#from the list of predictions made by the bagged trees.\n",
    "def bagging_predict(trees, row):\n",
    "\tpredictions = [predict(tree, row) for tree in trees]\n",
    "\treturn max(set(predictions), key=predictions.count)\n",
    " \n",
    "# Random Forest Algorithm\n",
    "#esponsible for creating the samples of the training dataset, training a decision tree on each,\n",
    "#then making predictions on the test dataset using the list of bagged trees.\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "\ttrees = list()\n",
    "\tfor i in range(n_trees):\n",
    "\t\tsample = subsample(train, sample_size)\n",
    "\t\ttree = build_tree(sample, max_depth, min_size, n_features)\n",
    "\t\ttrees.append(tree)\n",
    "\tpredictions = [bagging_predict(trees, row) for row in test]\n",
    "\treturn(predictions)\n",
    " \n",
    "# Test the random forest algorithm\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'sonar.all-data.csv'\n",
    "dataset = load_csv(filename)\n",
    "# convert string attributes to integers\n",
    "for i in range(0, len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "max_depth = 10\n",
    "min_size = 1\n",
    "sample_size = 1.0\n",
    "n_features = int(sqrt(len(dataset[0])-1))\n",
    "for n_trees in [1, 5, 10]:\n",
    "\tscores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
    "\tprint('Trees: %d' % n_trees)\n",
    "\tprint('Scores: %s' % scores)\n",
    "\tprint('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
