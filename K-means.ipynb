{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is computationally difficult(NP-hard); however, there are heuristic algorithm that are commonly employed and converge quickly to a local optimum.\n",
    "\n",
    "These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian Mixture Modeling. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of observations (x1, x2, …, xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k (≤ n) sets S = {S1, S2, …, Sk} so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:\n",
    "\n",
    "\\begin{equation*}\n",
    "argmin \\sum_{i=1}^{k}\\sum_{x in S_i}\\norm [2]{x - u_i}^2\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is a heuristic algorithm, there is no guarantee that it will converge to the global optimum, and the result may depend on the initial clusters. As the algorithm is usually very fast, it is common to run it multiple times with different starting conditions. However, in the worst case, k-means can be very slow to converge: in particular it has been shown that there exist certain point sets, even in 2 dimensions, on which k-means takes exponential time, that is 2Ω(n), to converge.[12] These point sets do not seem to arise in practice: this is corroborated by the fact that the smoothed running time of k-means is polynomial.[13]\n",
    "\n",
    "The \"assignment\" step is also referred to as expectation step, the \"update step\" as maximization step, making this algorithm a variant of the generalized expectation-maximization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If k and d (the dimension) are fixed, the problem can be exactly solved in time $${\\displaystyle O(n^{dk+1})}$$, where n is the number of entities to be clustered[19]\n",
    "Thus, a variety of heuristic algorithms such as Lloyd's algorithm given above are generally used.\n",
    "\n",
    "The running time of Lloyd's algorithm (and most variants) is O(nkdi),[20][21] where n is the number of d-dimensional vectors, k the number of clusters and i the number of iterations needed until convergence. \n",
    "\n",
    "Lloyd's algorithm is the standard approach for this problem, However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naive implementation very inefficient. Some implementations use caching and the triangle inequality in order to create bounds and accelerate Lloyd's algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three key features of k-means which make it efficient are often regarded as its biggest drawbacks:\n",
    "\n",
    "1. Euclidean distance is used as a metric and variance is used as a measure of cluster scatter. So it tends to create equal size clusters. while The Gaussian models used by the Expectation-maximization algorithm (which can be seen as a generalization of k-means) are more flexible here by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than k-means as well as correlated clusters (not in this example).and since it depends on euclidean distance, it dosen't apply to non-numerical data. Since it depends on euclidean distance, it's not a good metric in high dimensions\n",
    "\n",
    "\n",
    "[O]ur intuitions, which come from a three-dimensional world, often do not apply in high-dimensional ones. In high dimensions, most of the mass of a multivariate Gaussian distribution is not near the mean, but in an increasingly distant “shell” around it; and most of the volume of a high-dimensional orange is in the skin, not the pulp. If a constant number of examples is distributed uniformly in a high-dimensional hypercube, beyond some dimensionality most examples are closer to a face of the hypercube than to their nearest neighbor. And if we approximate a hypersphere by inscribing it in a hypercube,\n",
    ". It is the same situation happened in Nearest Neighbor. How to solve it?\n",
    "\n",
    "first, check the effective dimensions, do a PCA and see how the eigenvalue drops. \n",
    "second, Thus, for a given problem with a fixed (high) value for the dimensionality d, it may be preferable to use lower values of k. This means that the L1 distance metric (Manhattan distance metric) is the most preferable for high dimensional applications, followed by the Euclidean metric (L2). ...\n",
    "\n",
    "It's like the kNN balls are too sparse to be helpful at probing a smooth hyperplane. With higher dimensions they feel increasingly more lonely.\n",
    "\n",
    "On the other hand, methods like SVM have a global view and do much better.\n",
    "\n",
    "2. euclidean distance defined similairy has assumption that clusters are spatially grouped or spherical, iid guassian with same variance. it has similar variance at all dimensions\n",
    "\n",
    "3. number of clusters k is an input, cross validation to check reasonable right k is important.\n",
    "\n",
    "4. convergence to a local minimum may produce counterintuitive results\n",
    "\n",
    "5. no prior informatoin, bad for unbalanced data\n",
    "\n",
    "6. it is also sensetive to scale, unit measurement. Sometimes, non-linear scaling or transformation might also need to be applied. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means other application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationship with other ML algorithm\n",
    "\n",
    "1. Gaussian mixture model and EM algorithm\n",
    "\n",
    "the inherint assumption of Kmeans is that cluster is a sperical distributed. in reality, gaussian distribution is more common.\n",
    "\n",
    "In kmeans, the prior distribution assumption is that all cluster has the similar number of samples\n",
    "\n",
    "Kmeans use euclidean distance to measure the similarity, but in GMM, it is measured using posterior probability, covariance matrix can allow us to give priority to different dimension\n",
    "\n",
    "\n",
    "2. PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxDjYzcwS42A3M0vM/wNw43KYMfDvSwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9a85470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class KMeansClassifier():\n",
    "\t\"this is a k-means classifier\"\n",
    "\tdef __init__(self, k = 3, initCent = 'random', max_iter = 500):\n",
    "\t\tself._k = k\n",
    "\t\tself._initCent = initCent\n",
    "\t\tself._max_iter = max_iter\n",
    "\t\tself._clusterAssment = None\n",
    "\t\tself._labels = None\n",
    "\t\tself._sse = None\n",
    "\n",
    "\tdef _calEDist(self, arrA, arrB):\n",
    "\t\treturn np.math.sqrt(sum(np.power(arrA - arrB, 2)))\n",
    "\n",
    "\tdef _calMDist(self, arrA, arrB):\n",
    "\t\treturn sum(np.abs(arrA - arrB))\n",
    "\n",
    "\tdef _randCent(self, data_X, k):\n",
    "\t\t\"\"\"\n",
    "\t\toutput: k*m matrix of random centroid\n",
    "\t\t\"\"\"\n",
    "\t\tm = data_X.shape[1]\n",
    "\t\tcentroids = np.empty((k, m))\n",
    "\t\tfor j in range(m):\n",
    "\t\t\tminJ = min(data_X[:,j])\n",
    "\t\t\trangeJ = float(max(data_X[:, j] - minJ))\n",
    "\t\t\tcentroids[:, j] = (minJ + rangeJ*np.random.rand(k, 1)).flatten()\n",
    "\t\treturn centroids\n",
    "\n",
    "\tdef loadDataset(self, infile):\n",
    "\t\tdf = []\n",
    "\t\tfileIn = open(infile)  \n",
    "\t\tfor line in fileIn.readlines():  \n",
    "\t\t\tlineArr = line.strip().split()  \n",
    "\t\t\tdf.append([float(lineArr[0]), float(lineArr[1])])\n",
    "\t\tdf = pd.DataFrame(df)\n",
    "\t\treturn np.array(df).astype(np.float)\n",
    "\n",
    "\tdef fit(self, data_X):\n",
    "\t\t\"\"\"\n",
    "\t\tinput n*m matrix\n",
    "\t\t\"\"\"\n",
    "\t\tif not isinstance(data_X, np.ndarray) or isinstance(data_X, np.matrixlib.defmatrix.matrix):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tdata_X = np.asarray(data_X)\n",
    "\t\t\texcept:\n",
    "\t\t\t\traise TypeError(\"numpy.ndarray resuired for data_X\")\n",
    "\n",
    "\t\tn = data_X.shape[0]\n",
    "\t\tself._clusterAssment = np.zeros((n, 2))\n",
    "\t\tif self._initCent == 'random':\n",
    "\t\t\tself._centroids = self._randCent(data_X, self._k)\n",
    "\n",
    "\t\tclusterChanged = True\n",
    "\t\tfor _ in range(self._max_iter):\n",
    "\t\t\tclusterChanged = False\n",
    "\t\t\t# loop over each data point\n",
    "\t\t\tfor i in range(n):\n",
    "\t\t\t\tminDist = np.inf\n",
    "\t\t\t\tminIndex = -1\n",
    "\t\t\t\tfor j in range(self._k):\n",
    "\t\t\t\t\t# loop over each centroid\n",
    "\t\t\t\t\tarrA = self._centroids[j,:]\n",
    "\t\t\t\t\tarrB = data_X[i,:]\n",
    "\t\t\t\t\tdistJI = self._calEDist(arrA, arrB)\n",
    "\t\t\t\t\tif distJI < minDist:\n",
    "\t\t\t\t\t\tminDist = distJI\n",
    "\t\t\t\t\t\tminIndex = j\n",
    "\t\t\t\tif self._clusterAssment[i, 0] != minIndex or self._clusterAssment[i, 1] > minDist**2:\n",
    "\t\t\t\t\tclusterChanged = True\n",
    "\t\t\t\t\tself._clusterAssment[i, :] = minIndex, minDist**2\n",
    "\t\t\tif not clusterChanged:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tfor i in range(self._k):\n",
    "\t\t\t\tindex_all = self._clusterAssment[:, 0]\n",
    "\t\t\t\tvalue = np.nonzero(index_all == i)\n",
    "\t\t\t\tptsInClust = data_X[value[0]]\n",
    "\t\t\t\tself._centroids[i,:] = np.mean(ptsInClust, axis = 0)\n",
    "\n",
    "\t\tself._labels = self._clusterAssment[: , 0]\n",
    "\t\tself._sse = sum(self._clusterAssment[:, 1])\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\tif not isinstance(X, np.ndarray):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tX = np.asarray(X)\n",
    "\t\t\texcept:\n",
    "\t\t\t\traise TypeError(\"numpy.ndarray required for X\")\n",
    "\n",
    "\t\tn = X.shape[0]\n",
    "\t\tpreds = np.zeros((n,))\n",
    "\t\tfor i in range(n):\n",
    "\t\t\tminDist = np.inf\n",
    "\t\t\tfor j in range(self._k):\n",
    "\t\t\t\tdistJI = self._calEDist(X[i,:], self._centroids[j,])\n",
    "\t\t\t\tif distJI < minDist:\n",
    "\t\t\t\t\tminDist = distJI\n",
    "\t\t\t\t\tpreds[i] = j\n",
    "\t\treturn preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\tk = 3\n",
    "\tclf = KMeansClassifier(k)\n",
    "\tdata_X = clf.loadDataset(\"C:/Users/zihao.zhang/Desktop/some questions/some-questions/kmenastestdata.txt\")\n",
    "\tclf.fit(data_X)\n",
    "\tcents = clf._centroids\n",
    "\tlabels = clf._labels\n",
    "\tsse = clf._sse\n",
    "\tcolors = ['b','g','r','k','c','m','y','#e24fff','#524C90','#845868']\n",
    "\tfor i in range(k):\n",
    "\t\tindex = np.nonzero(labels==i)[0]\n",
    "\t\tx0 = data_X[index, 0]\n",
    "\t\tx1 = data_X[index, 1]\n",
    "\t\ty_i = i\n",
    "\t\tfor j in range(len(x0)):\n",
    "\t\t\tplt.text(x0[j], x1[j], str(y_i), color=colors[i], \\\n",
    "\t\t\t\t\t\tfontdict={'weight': 'bold', 'size': 6})\n",
    "\t\tplt.scatter(cents[i,0],cents[i,1],marker='x',color=colors[i],\\\n",
    "\t\t\t\t\tlinewidths=7)\n",
    "\t\n",
    "\tplt.title(\"SSE={:.2f}\".format(sse))\n",
    "\tplt.axis([-7,7,-7,7])\n",
    "\toutname = \"./result/k_clusters\" + str(k) + \".png\"\n",
    "\tplt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
